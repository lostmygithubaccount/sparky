{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Load sparkmagic before start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsetdata = 'noaa-isd-files'\n",
    "data_url = 'https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather'\n",
    "\n",
    "if dsetdata not in ws.datasets:\n",
    "    os.system('sudo chmod 777 /mnt')\n",
    "    for year in range(2008, 2020+1):\n",
    "        ds = Dataset.File.from_files(f'{data_url}/year={year}/month=*/*.parquet', validate=False)\n",
    "        print('Downloading...')\n",
    "        %time ds.download(f'/mnt/data/isd/year={year}', overwrite=True)\n",
    "    print('Uploading...')\n",
    "    %time ws.get_default_datastore().upload('/mnt/data/isd', '/noaa-isd', show_progress=False)\n",
    "    ds = Dataset.File.from_files((ws.get_default_datastore(), '/noaa-isd/**/*.parquet'))\n",
    "    ds = ds.register(ws, dsetdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsetdata = 'noaa-isd-tabular'\n",
    "\n",
    "if dsetdata not in ws.datasets:\n",
    "    ds = Dataset.Tabular.from_parquet_files((ws.get_default_datastore(), '/noaa-isd/**/*.parquet'))\n",
    "    ds = ds.register(ws, dsetdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code BQSA4WNQL to authenticate.\n",
      "Starting Spark application ...\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# use any Synapse workspace/Spark pool you can access with AAD \n",
    "%spark start --workspace sparky --sparkpool sparky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark config\n",
    "{\n",
    "    \"driverMemory\":\"128g\",\n",
    "    \"driverCores\":16,\n",
    "    \"executorMemory\":\"8g\",\n",
    "    \"executorCores\":1,\n",
    "    \"numExecutors\":200\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace.create(name='AzureML', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='cody-westus2-rg')"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# get workspace\n",
    "from azureml.core import Workspace\n",
    "subscription_id = '6560575d-fa06-4e7d-95fb-f962e74efd7a'\n",
    "resource_group = 'cody-westus2-rg'\n",
    "workspace_name = 'AzureML'\n",
    "\n",
    "ws = Workspace(subscription_id, resource_group, workspace_name)\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"source\": [\n",
      "    \"('workspaceblobstore', 'noaa-isd/**/*.parquet')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\",\n",
      "    \"ReadParquetFile\",\n",
      "    \"DropColumns\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"02c60e72-49c6-482c-9672-2dac6c5d7439\",\n",
      "    \"name\": \"noaa-isd-tabular\",\n",
      "    \"version\": 1,\n",
      "    \"workspace\": \"Workspace.create(name='AzureML', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='cody-westus2-rg')\"\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "%%spark \n",
    "\n",
    "ds = ws.datasets['noaa-isd-tabular']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o135.getFiles.\n",
      ": java.lang.RuntimeException: Unable to get service host.\n",
      "Workspace ID: \n",
      "Workspace ID Program Error: Could not find valid SPARK_HOME while searching ['/var/yarn-nm/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1584502209143_0002', '/usr/hdp/2.6.99.201-0/spark2/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib']\n",
      "Service Url Program Error: Could not find valid SPARK_HOME while searching ['/var/yarn-nm/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1584502209143_0002', '/usr/hdp/2.6.99.201-0/spark2/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib']\n",
      "\tat com.microsoft.dprep.integration.azureml.ServiceDiscovery$.getHistoryUri(ServiceDiscovery.scala:37)\n",
      "\tat com.microsoft.dprep.integration.azureml.DatastoreServiceResolver.resolve(DatastoreResolver.scala:47)\n",
      "\tat com.microsoft.dprep.integration.azureml.CachedDatastoreResolver.resolve(DatastoreResolver.scala:26)\n",
      "\tat com.microsoft.dprep.integration.azureml.DatastoreMounter.mount(DatastoreMounter.scala:24)\n",
      "\tat com.microsoft.dprep.execution.Storage$.mount(Storage.scala:57)\n",
      "\tat com.microsoft.dprep.execution.Storage$.expandHdfsPath(Storage.scala:36)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$$anonfun$1.apply(GetFilesExecutor.scala:14)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$$anonfun$1.apply(GetFilesExecutor.scala:12)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$.getFiles(GetFilesExecutor.scala:12)\n",
      "\tat com.microsoft.dprep.execution.LariatDataset$.getFiles(LariatDataset.scala:25)\n",
      "\tat com.microsoft.dprep.execution.PySparkExecutor.getFiles(PySparkExecutor.scala:202)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/azureml/data/_loggerfactory.py\", line 78, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/azureml/data/tabular_dataset.py\", line 153, in to_spark_dataframe\n",
      "    return _try_execute(dataflow.to_spark_dataframe)\n",
      "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/azureml/data/dataset_error_handling.py\", line 85, in _try_execute\n",
      "    raise DatasetExecutionError(str(e))\n",
      "azureml.data.dataset_error_handling.DatasetExecutionError: An error occurred while calling o135.getFiles.\n",
      ": java.lang.RuntimeException: Unable to get service host.\n",
      "Workspace ID: \n",
      "Workspace ID Program Error: Could not find valid SPARK_HOME while searching ['/var/yarn-nm/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1584502209143_0002', '/usr/hdp/2.6.99.201-0/spark2/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib']\n",
      "Service Url Program Error: Could not find valid SPARK_HOME while searching ['/var/yarn-nm/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1584502209143_0002', '/usr/hdp/2.6.99.201-0/spark2/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib']\n",
      "\tat com.microsoft.dprep.integration.azureml.ServiceDiscovery$.getHistoryUri(ServiceDiscovery.scala:37)\n",
      "\tat com.microsoft.dprep.integration.azureml.DatastoreServiceResolver.resolve(DatastoreResolver.scala:47)\n",
      "\tat com.microsoft.dprep.integration.azureml.CachedDatastoreResolver.resolve(DatastoreResolver.scala:26)\n",
      "\tat com.microsoft.dprep.integration.azureml.DatastoreMounter.mount(DatastoreMounter.scala:24)\n",
      "\tat com.microsoft.dprep.execution.Storage$.mount(Storage.scala:57)\n",
      "\tat com.microsoft.dprep.execution.Storage$.expandHdfsPath(Storage.scala:36)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$$anonfun$1.apply(GetFilesExecutor.scala:14)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$$anonfun$1.apply(GetFilesExecutor.scala:12)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$.getFiles(GetFilesExecutor.scala:12)\n",
      "\tat com.microsoft.dprep.execution.LariatDataset$.getFiles(LariatDataset.scala:25)\n",
      "\tat com.microsoft.dprep.execution.PySparkExecutor.getFiles(PySparkExecutor.scala:202)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df = ds.to_spark_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Session Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override Session Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark config\n",
    "{\n",
    "    \"driverMemory\":\"8g\",\n",
    "    \"driverCores\":2,\n",
    "    \"executorMemory\":\"8g\",\n",
    "    \"executorCores\":2,\n",
    "    \"numExecutors\":2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: RDD operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "import time\n",
    "b=sc.broadcast([3,5]) #Creating a broadcast variable available on all executors\n",
    "a=sc.accumulator(0)   #Creating an accumulator for adding values across executors\n",
    "RDD0=sc.parallelize([y for y in range(0,5)]) #RDD from input python collection\n",
    "RDD2=sc.parallelize([z for z in range(10,15)])\n",
    "RDD1=RDD0.cartesian(RDD2) \n",
    "cached=RDD2.cache() #Testing cached RDD\n",
    "RDD22=RDD1.map(lambda x:x[0]+x[1]+b.value[0])\n",
    "RDD3=RDD22.repartition(5) # To trigger a new stage.\n",
    "RDD4=RDD2.map(lambda x: 3*x-b.value[0])\n",
    "RDD5=RDD3.filter(lambda x:x%2==0)\n",
    "RDD6=RDD4.filter(lambda x:x%2!=0)\n",
    "RDD7=RDD5.cartesian(RDD6)\n",
    "RDD8=RDD7.flatMap(lambda x: [x[i] for i in range(0,2)])\n",
    "RDD9=RDD8.union(cached)\n",
    "ans=RDD9.reduce(lambda x,y: x+y) # Doing a simple sum on the random data.\n",
    "print(ans)\n",
    "def f(x):\n",
    "    global a\n",
    "    time.sleep(0.7) #Making the job run a little longer\n",
    "    a+=x\n",
    "RDD9.foreach(f)\n",
    "print(a.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Spark Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "partitions = 10\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = sc.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "saveOutput": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
