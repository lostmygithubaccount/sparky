{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "* N Synapse workspaces can be linked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = ws.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Synapse workspace\n",
    "\n",
    "Should we allow linking to a Synapse workspace from Azure ML, or enforce one-way linking from Synapse only?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synapse_params = \n",
    "{\n",
    "    workspace_name  = 'my_synapse_ws',      # required input\n",
    "    resource_group  = ws.resource_group,    # default to same as AML Workspace \n",
    "    subscription_id = ws.subscription_id    # default to same as AML Workspace\n",
    "    **AUTH_OPTIONS                          # details pending design\n",
    "}\n",
    "\n",
    "# minimal API\n",
    "ws.link_synapse_workspace(**synapse_params) # all Datastores and Compute Targets imported \n",
    "\n",
    "# full API\n",
    "ws.link_synapse_workspace(**synapse_params,          # required inputs\n",
    "                          linked_services    = True, # optional - default to True\n",
    "                          spark_pools        = True, # optional - default to True \n",
    "                         )\n",
    "\n",
    "###############################################\n",
    "######## OUT OF SCOPE FOR MANGANESE ###########\n",
    "###############################################\n",
    "\n",
    "linked_services_map = \n",
    "{\n",
    "    'my_linked_service': \n",
    "    {\n",
    "        'datastores': \n",
    "        {\n",
    "            'BlobContainers': ['container1', 'container2'],\n",
    "            'FileSystems': ['fs1', 'fs2']\n",
    "        }\n",
    "    }\n",
    "    'my_other_linked_service': \n",
    "    {\n",
    "        'datastores': \n",
    "        {\n",
    "            'SQLDBs': ['MySQLDB1', 'MySQLDB2']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "spark_pools = ['pool1', 'pool2', 'pool3']\n",
    "\n",
    "# full API\n",
    "ws.link_synapse_workspace(**synapse_params,                         # required inputs\n",
    "                          linked_services    = linked_services_map, # select specific datastores\n",
    "                          spark_pools        = spark_pools          # select specific compute targets \n",
    "                         )\n",
    "\n",
    "# register datastores from Synapse into Azure ML\n",
    "ws.register_synapse_datastores(linked_services_map)\n",
    "\n",
    "# unregister Synapse datastores from Azure ML \n",
    "ws.unregister_synapse_datastores(linked_services_map)\n",
    "\n",
    "# attach spark pools from Synapse into Azure ML\n",
    "ws.attach_synapse_spark_pools(spark_pools) \n",
    "\n",
    "# detach Synapse spark pools from Azure ML\n",
    "ws.detach_syanpse_spark_pools(spark_pools) \n",
    "\n",
    "###############################################\n",
    "######## OUT OF SCOPE FOR MANGANESE ###########\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Compute and Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.Tabular.from_delimited_files(path = [(blob_datastore, 'data/my_data/*.parquet')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/dataprep.py\n",
    "\n",
    "from azureml.core import Dataset, Run\n",
    "\n",
    "run     = Run.get_context()\n",
    "dataset = run.input_datasets['my_data']\n",
    "df      = dataset.to_spark_dataframe()\n",
    "\n",
    "### data preparation and/or training code\n",
    "df['temperature'] = df['temperature']*(9/5) + 32\n",
    "\n",
    "\n",
    "###############################################\n",
    "################## NEW API ####################\n",
    "###############################################\n",
    "\n",
    "# minimal API\n",
    "new_dataset = Dataset.Tabular.from_spark_dataframe(df) # new API\n",
    "\n",
    "# full API\n",
    "new_dataset = Dataset.Tabular.from_spark_dataframe(df,  # required inputs\n",
    "                    # optional parameters w/ sensible defaults \n",
    "                    datastore   = ws.get_default_datastore(),\n",
    "                    target_path = f'from_spark_df/{GUID}/{version}/data/part-*.{compression}.parquet',\n",
    "                    compression = 'lz4',\n",
    "                    overwrite   = False,\n",
    "                    new_version = True\n",
    "                    )\n",
    "\n",
    "new_dataset = new_dataset.register(ws, name) # existing API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifics for runs on Synapse tbd\n",
    "\n",
    "Pending LT/Python SDK feedback on best way to expose Synapse compute targets, and whether there are technical considerations for adding new concepts such as a `SynapseEstimator` or `SynapseStep`. Regardless, Synapse Spark compute targets should be usable in:\n",
    "\n",
    "* regular Python script runs\n",
    "* Pipelines as a step (including Designer)\n",
    "* notebooks for interactive PySpark on datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig, RunConfiguration, Experiment, CondaDependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pyspark framework\n",
    "synapse_config = RunConfiguration(framework=\"pyspark\")\n",
    "\n",
    "# Set compute target to the attached Synapse Spark cluster\n",
    "syanpse_config.target = ws.compute_targets['SynapsePool']\n",
    "\n",
    "# get the environment - use Environments to tailor as needed \n",
    "syanpse_config.environment = ws.environments['Synapse-AzureML-Spark']  # new curated environment w/ default packages\n",
    "\n",
    "# create ScriptRunConfig object  \n",
    "config = ScriptRunConfig(source_directory='.', script='dataprep.py', run_config=run_SynapseSpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create experiment, submit run \n",
    "exp = Experiment(ws, 'synapse_dprep')\n",
    "run = exp.submit(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails.show(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1   = SynapseStep(name             = 'synapseStep'\n",
    "                      source_directory = 'scripts',\n",
    "                      entry_script     = 'dataprep.py',\n",
    "                      inputs           = [dataset.as_named_input('my_data')],\n",
    "                      compute_target   = synapse_pool,\n",
    "                      workers          = 100,\n",
    "                      allow_resuse     = True\n",
    "                     )\n",
    "\n",
    "step2   = ......\n",
    "\n",
    "pipeline = Pipeline([step1, step2]).publish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
