{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Setup"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data and explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.datasets['noaa-isd-tabular']\n",
    "df = ds.sample(1000).to_pandas_dataframe()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%synapse start --compute-target sparky --environment 'AzureML-Synapse' # compute target name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%synapse restart --environment 'MyCustomEnvironment' # restart with new spark/python Environment changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%synapse # default to PySpark\n",
    "\n",
    "# get workspace\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# automagic\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%synapse \n",
    "\n",
    "ds = ws.datasets['noaa-isd-tabular']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%synapse\n",
    "\n",
    "df = ds.to_spark_dataframe()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%synapse\n",
    "\n",
    "# basic EDA\n",
    "df.summary().eval() # equivalent to Pandas dataframe df.describe()\n",
    "\n",
    "## start data prep code\n",
    "X = df.drop('target')\n",
    "Y = df['target']\n",
    "\n",
    "X = X.fillna('0').groupby(df['datetime']).mean().filter(df['temperature'] < 50)\n",
    "Y = Y.fillna(NaN).groupby(df['datetime']).mean()\n",
    "## end data prep code\n",
    "\n",
    "# save state in cloud as Dataset\n",
    "dsX = Dataset.Tabular.from_spark_df(X).register(ws, 'noaa-isd-X')\n",
    "dsY = Dataset.Tabular.from_spark_df(Y).register(ws, 'noaa-isd-Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%synapse \n",
    "\n",
    "# write to detla table in temp storage for use in Synapse Spark/Dask contexts\n",
    "df.write.format('delta').save(f'/tmp/noaa-isd') \n",
    "\n",
    "# save as a delta table in ADLS gen2 for use in almost any Azure Compute context\n",
    "df.write.format('delta').save(f'abfs://{container_name}@{account_name}.dfs.core.windows.net/datasets/noaa-isd/')\n",
    "\n",
    "# save as a dataset \n",
    "df.to_azureml_dataset('noaa-isd') # intentionally incorrect API \n",
    "\n",
    "# make a sql-readable table\n",
    "spark.sql(\"CREATE TABLE noaaisd USING DELTA LOCATION '/tmp/noaa-isd'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%synapse sql\n",
    "\n",
    "# use SQL syntax to query/visulize data\n",
    "SELECT * FROM noaaisd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "script_folder = './pytorch-dnn'\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n",
    "exp = Experiment(workspace=ws, name='keras-mnist')\n",
    "\n",
    "script_params = {\n",
    "    '--epochs': 1000,\n",
    "    '--final_layer': 'sigmoid',\n",
    "    '--X_dataset': ws.datasets['noaa-isd-X'],\n",
    "    '--Y_dataset': ws.datasets['noaa-isd-Y']\n",
    "}\n",
    "\n",
    "est = Estimator(source_directory = '.', \n",
    "                script_params    = script_params, \n",
    "                compute_target   = ws.compute_targets['gpu-cluster'],\n",
    "                entry_script     = 'keras_train.py', \n",
    "                use_docker       = False\n",
    "                )\n",
    "\n",
    "run = exp.submit(est)\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Session"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%synapse stop"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "saveOutput": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}