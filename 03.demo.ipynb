{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Load sparkmagic before start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure ML Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Synapse ADLS Gen2 Account as a Datastore\n",
    "\n",
    "Let's register the Synapse workspace's default ADLS Gen2 account as a datastore and make it the default for Azure ML. This will make life easier for working between Synapse and Azure ML with this specific workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datastore.register_azure_blob_container(ws, 'data4synapse', 'default', 'data4synapse', account_key = 'jZg362stjKP6EjMdXKCDH7OgnqOCQ2qPDTry5CbRwv6PB0DlJWBssdgvfQbi5GUy3dO5pgv7jRv4qtw0ngccwg==')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsetdata = 'noaa-isd-files'\n",
    "data_url = 'https://azureopendatastorage.blob.core.windows.net/isdweatherdatacontainer/ISDWeather'\n",
    "\n",
    "if dsetdata not in ws.datasets:\n",
    "    os.system('sudo chmod 777 /mnt')\n",
    "    for year in range(2008, 2020+1):\n",
    "        ds = Dataset.File.from_files(f'{data_url}/year={year}/month=*/*.parquet', validate=False)\n",
    "        print('Downloading...')\n",
    "        %time ds.download(f'/mnt/data/isd/year={year}', overwrite=True)\n",
    "    print('Uploading...')\n",
    "    %time ws.get_default_datastore().upload('/mnt/data/isd', '/noaa-isd', show_progress=False)\n",
    "    ds = Dataset.File.from_files((ws.get_default_datastore(), '/noaa-isd/**/*.parquet'))\n",
    "    ds = ds.register(ws, dsetdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsetdata = 'noaa-isd-tabular'\n",
    "\n",
    "if dsetdata not in ws.datasets:\n",
    "    ds = Dataset.Tabular.from_parquet_files((ws.get_default_datastore(), '/noaa-isd/**/*.parquet'))\n",
    "    ds = ds.register(ws, dsetdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code BBJTKLSYR to authenticate.\n",
      "Starting Spark application ...\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# use any Synapse workspace/Spark pool you can access with AAD \n",
    "%spark start --workspace sparky --sparkpool sparky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%spark config # TODO: learn how to configure Spark sessions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace.create(name='AzureML', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='cody-westus2-rg')"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# get workspace\n",
    "from azureml.core import Workspace\n",
    "subscription_id = '6560575d-fa06-4e7d-95fb-f962e74efd7a'\n",
    "resource_group = 'cody-westus2-rg'\n",
    "workspace_name = 'AzureML'\n",
    "\n",
    "ws = Workspace(subscription_id, resource_group, workspace_name)\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"source\": [\n",
      "    \"('data4synapse', 'noaa-isd/**/*.parquet')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\",\n",
      "    \"ReadParquetFile\",\n",
      "    \"DropColumns\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"3254db94-0c1d-416e-be99-98061ae4f479\",\n",
      "    \"name\": \"noaa-isd-tabular\",\n",
      "    \"version\": 1,\n",
      "    \"workspace\": \"Workspace.create(name='AzureML', subscription_id='6560575d-fa06-4e7d-95fb-f962e74efd7a', resource_group='cody-westus2-rg')\"\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "%%spark \n",
    "\n",
    "ds = ws.datasets['noaa-isd-tabular']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o135.getFiles.\n",
      ": java.lang.RuntimeException: Unable to get service host.\n",
      "Workspace ID: \n",
      "Workspace ID Program Error: Could not find valid SPARK_HOME while searching ['/var/yarn-nm/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1584547190049_0003', '/usr/hdp/2.6.99.201-0/spark2/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib']\n",
      "Service Url Program Error: Could not find valid SPARK_HOME while searching ['/var/yarn-nm/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1584547190049_0003', '/usr/hdp/2.6.99.201-0/spark2/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib']\n",
      "\tat com.microsoft.dprep.integration.azureml.ServiceDiscovery$.getHistoryUri(ServiceDiscovery.scala:37)\n",
      "\tat com.microsoft.dprep.integration.azureml.DatastoreServiceResolver.resolve(DatastoreResolver.scala:47)\n",
      "\tat com.microsoft.dprep.integration.azureml.CachedDatastoreResolver.resolve(DatastoreResolver.scala:26)\n",
      "\tat com.microsoft.dprep.integration.azureml.DatastoreMounter.mount(DatastoreMounter.scala:24)\n",
      "\tat com.microsoft.dprep.execution.Storage$.mount(Storage.scala:57)\n",
      "\tat com.microsoft.dprep.execution.Storage$.expandHdfsPath(Storage.scala:36)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$$anonfun$1.apply(GetFilesExecutor.scala:14)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$$anonfun$1.apply(GetFilesExecutor.scala:12)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$.getFiles(GetFilesExecutor.scala:12)\n",
      "\tat com.microsoft.dprep.execution.LariatDataset$.getFiles(LariatDataset.scala:25)\n",
      "\tat com.microsoft.dprep.execution.PySparkExecutor.getFiles(PySparkExecutor.scala:202)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/azureml/data/_loggerfactory.py\", line 78, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/azureml/data/tabular_dataset.py\", line 153, in to_spark_dataframe\n",
      "    return _try_execute(dataflow.to_spark_dataframe)\n",
      "  File \"/home/trusted-service-user/cluster-env/env/lib/python3.6/site-packages/azureml/data/dataset_error_handling.py\", line 85, in _try_execute\n",
      "    raise DatasetExecutionError(str(e))\n",
      "azureml.data.dataset_error_handling.DatasetExecutionError: An error occurred while calling o135.getFiles.\n",
      ": java.lang.RuntimeException: Unable to get service host.\n",
      "Workspace ID: \n",
      "Workspace ID Program Error: Could not find valid SPARK_HOME while searching ['/var/yarn-nm/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1584547190049_0003', '/usr/hdp/2.6.99.201-0/spark2/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib']\n",
      "Service Url Program Error: Could not find valid SPARK_HOME while searching ['/var/yarn-nm/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1584547190049_0003', '/usr/hdp/2.6.99.201-0/spark2/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib/pyspark.zip/pyspark', '/opt/spark/python/lib']\n",
      "\tat com.microsoft.dprep.integration.azureml.ServiceDiscovery$.getHistoryUri(ServiceDiscovery.scala:37)\n",
      "\tat com.microsoft.dprep.integration.azureml.DatastoreServiceResolver.resolve(DatastoreResolver.scala:47)\n",
      "\tat com.microsoft.dprep.integration.azureml.CachedDatastoreResolver.resolve(DatastoreResolver.scala:26)\n",
      "\tat com.microsoft.dprep.integration.azureml.DatastoreMounter.mount(DatastoreMounter.scala:24)\n",
      "\tat com.microsoft.dprep.execution.Storage$.mount(Storage.scala:57)\n",
      "\tat com.microsoft.dprep.execution.Storage$.expandHdfsPath(Storage.scala:36)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$$anonfun$1.apply(GetFilesExecutor.scala:14)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$$anonfun$1.apply(GetFilesExecutor.scala:12)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
      "\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)\n",
      "\tat com.microsoft.dprep.execution.executors.GetFilesExecutor$.getFiles(GetFilesExecutor.scala:12)\n",
      "\tat com.microsoft.dprep.execution.LariatDataset$.getFiles(LariatDataset.scala:25)\n",
      "\tat com.microsoft.dprep.execution.PySparkExecutor.getFiles(PySparkExecutor.scala:202)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df = ds.to_spark_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'abfss://default@data4synapse.dfs.core.windows.net/noaa-isd/'"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "account_name = dstore.account_name\n",
    "container_name = dstore.container_name\n",
    "relative_path = 'noaa-isd/'\n",
    "\n",
    "adls_path = f'abfss://{container_name}@{account_name}.dfs.core.windows.net/{relative_path}'\n",
    "adls_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------------------+--------+---------+---------+---------+---------+-----------+--------------+-------------+-----------------------+--------------------+----------+-----------+---------+------------------+---------------+------------+----+---+-------+\n",
      "|  usaf| wban|           datetime|latitude|longitude|elevation|windAngle|windSpeed|temperature|seaLvlPressure|cloudCoverage|presentWeatherIndicator|pastWeatherIndicator|precipTime|precipDepth|snowDepth|       stationName|countryOrRegion|         p_k|year|day|version|\n",
      "+------+-----+-------------------+--------+---------+---------+---------+---------+-----------+--------------+-------------+-----------------------+--------------------+----------+-----------+---------+------------------+---------------+------------+----+---+-------+\n",
      "|999999|23909|2020-01-05 21:50:00|  37.634|  -91.723|    365.0|     null|     null|       12.0|          null|         null|                   null|                null|      null|       null|     null|        SALEM 10 W|             US|999999-23909|2020|  5|    1.0|\n",
      "|999999|04223|2020-01-15 23:30:00|  48.541| -121.446|    124.0|     null|     null|       -2.7|          null|         null|                   null|                null|      null|       null|     null| DARRINGTON 21 NNE|             US|999999-04223|2020| 15|    1.0|\n",
      "|999999|13301|2020-01-08 13:55:00|  39.867|  -93.147|    254.0|     null|     null|       -5.4|          null|         null|                   null|                null|      null|       null|     null|CHILLICOTHE 22 ENE|             US|999999-13301|2020|  8|    1.0|\n",
      "|999999|13301|2020-01-21 22:40:00|  39.867|  -93.147|    254.0|     null|     null|       -5.0|          null|         null|                   null|                null|      null|       null|     null|CHILLICOTHE 22 ENE|             US|999999-13301|2020| 21|    1.0|\n",
      "|999999|04223|2020-01-09 00:55:00|  48.541| -121.446|    124.0|     null|     null|        2.3|          null|         null|                   null|                null|      null|       null|     null| DARRINGTON 21 NNE|             US|999999-04223|2020|  9|    1.0|\n",
      "|999999|12987|2020-01-21 13:00:00|  26.526|  -98.063|     20.0|     null|      1.5|        8.1|          null|         null|                   null|                null|       1.0|        0.0|     null|   EDINBURG 17 NNE|             US|999999-12987|2020| 21|    1.0|\n",
      "|999999|04223|2020-01-24 08:35:00|  48.541| -121.446|    124.0|     null|     null|        3.0|          null|         null|                   null|                null|      null|       null|     null| DARRINGTON 21 NNE|             US|999999-04223|2020| 24|    1.0|\n",
      "|999999|23803|2020-01-05 01:30:00|  34.822|  -89.435|    148.0|     null|     null|       -1.6|          null|         null|                   null|                null|      null|       null|     null| HOLLY SPRINGS 4 N|             US|999999-23803|2020|  5|    1.0|\n",
      "|999999|04236|2020-01-07 09:35:00|  44.419| -123.326|     95.0|     null|     null|        9.1|          null|         null|                   null|                null|      null|       null|     null|  CORVALLIS 10 SSW|             US|999999-04236|2020|  7|    1.0|\n",
      "|999999|13301|2020-01-20 10:25:00|  39.867|  -93.147|    254.0|     null|     null|      -14.9|          null|         null|                   null|                null|      null|       null|     null|CHILLICOTHE 22 ENE|             US|999999-13301|2020| 20|    1.0|\n",
      "|999999|04236|2020-01-27 22:45:00|  44.419| -123.326|     95.0|     null|     null|       10.1|          null|         null|                   null|                null|      null|       null|     null|  CORVALLIS 10 SSW|             US|999999-04236|2020| 27|    1.0|\n",
      "|999999|25379|2020-01-25 19:15:00|  57.057| -135.327|     24.0|     null|     null|        0.1|          null|         null|                   null|                null|      null|       null|     null|        SITKA 1 NE|             US|999999-25379|2020| 25|    1.0|\n",
      "|999999|23803|2020-01-27 15:15:00|  34.822|  -89.435|    148.0|     null|     null|        9.1|          null|         null|                   null|                null|      null|       null|     null| HOLLY SPRINGS 4 N|             US|999999-23803|2020| 27|    1.0|\n",
      "|999999|25379|2020-01-02 08:25:00|  57.057| -135.327|     24.0|     null|     null|       -1.9|          null|         null|                   null|                null|      null|       null|     null|        SITKA 1 NE|             US|999999-25379|2020|  2|    1.0|\n",
      "|999999|04236|2020-01-15 04:15:00|  44.419| -123.326|     95.0|     null|     null|       -0.4|          null|         null|                   null|                null|      null|       null|     null|  CORVALLIS 10 SSW|             US|999999-04236|2020| 15|    1.0|\n",
      "|999999|12987|2020-01-01 02:25:00|  26.526|  -98.063|     20.0|     null|     null|       14.7|          null|         null|                   null|                null|      null|       null|     null|   EDINBURG 17 NNE|             US|999999-12987|2020|  1|    1.0|\n",
      "|999999|04223|2020-01-31 19:55:00|  48.541| -121.446|    124.0|     null|     null|        5.9|          null|         null|                   null|                null|      null|       null|     null| DARRINGTON 21 NNE|             US|999999-04223|2020| 31|    1.0|\n",
      "|999999|23907|2020-01-02 20:50:00|  30.622|  -98.085|    415.0|     null|     null|       13.5|          null|         null|                   null|                null|      null|       null|     null|      AUSTIN 33 NW|             US|999999-23907|2020|  2|    1.0|\n",
      "|999999|23909|2020-01-31 13:15:00|  37.634|  -91.723|    365.0|     null|     null|        2.0|          null|         null|                   null|                null|      null|       null|     null|        SALEM 10 W|             US|999999-23909|2020| 31|    1.0|\n",
      "|999999|25379|2020-01-09 20:05:00|  57.057| -135.327|     24.0|     null|     null|       -4.7|          null|         null|                   null|                null|      null|       null|     null|        SITKA 1 NE|             US|999999-25379|2020|  9|    1.0|\n",
      "+------+-----+-------------------+--------+---------+---------+---------+---------+-----------+--------------+-------------+-----------------------+--------------------+----------+-----------+---------+------------------+---------------+------------+----+---+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df = sqlContext.read.parquet(f'{adls_path}/year=*/month=*/*.parquet')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Session Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override Session Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark config\n",
    "{\n",
    "    \"driverMemory\":\"8g\",\n",
    "    \"driverCores\":2,\n",
    "    \"executorMemory\":\"8g\",\n",
    "    \"executorCores\":2,\n",
    "    \"numExecutors\":2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: RDD operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "import time\n",
    "b=sc.broadcast([3,5]) #Creating a broadcast variable available on all executors\n",
    "a=sc.accumulator(0)   #Creating an accumulator for adding values across executors\n",
    "RDD0=sc.parallelize([y for y in range(0,5)]) #RDD from input python collection\n",
    "RDD2=sc.parallelize([z for z in range(10,15)])\n",
    "RDD1=RDD0.cartesian(RDD2) \n",
    "cached=RDD2.cache() #Testing cached RDD\n",
    "RDD22=RDD1.map(lambda x:x[0]+x[1]+b.value[0])\n",
    "RDD3=RDD22.repartition(5) # To trigger a new stage.\n",
    "RDD4=RDD2.map(lambda x: 3*x-b.value[0])\n",
    "RDD5=RDD3.filter(lambda x:x%2==0)\n",
    "RDD6=RDD4.filter(lambda x:x%2!=0)\n",
    "RDD7=RDD5.cartesian(RDD6)\n",
    "RDD8=RDD7.flatMap(lambda x: [x[i] for i in range(0,2)])\n",
    "RDD9=RDD8.union(cached)\n",
    "ans=RDD9.reduce(lambda x,y: x+y) # Doing a simple sum on the random data.\n",
    "print(ans)\n",
    "def f(x):\n",
    "    global a\n",
    "    time.sleep(0.7) #Making the job run a little longer\n",
    "    a+=x\n",
    "RDD9.foreach(f)\n",
    "print(a.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Spark Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark\n",
    "\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "partitions = 10\n",
    "n = 100000 * partitions\n",
    "\n",
    "def f(_):\n",
    "    x = random() * 2 - 1\n",
    "    y = random() * 2 - 1\n",
    "    return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "count = sc.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "saveOutput": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
