{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Datasets in Azure ML Synapse Spark\n",
    "\n",
    "This mockup notebook covers using Datasets in Azure ML Synapse Spark for Tabular, File, 'local', and 'remote' usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "Either use an existing Dataset created from a Datastore in the UI, Python SDK, etc. or create one from a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ... # dataframe in Synapse Spark session or otherwise\n",
    "\n",
    "# create Azure ML Dataset \n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "ws = Workspace.from_config() # see https://azureml/workspace \n",
    "dset = Dataset.Tabular.from_spark_dataframe(df).register('my-data', ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataframe\n",
    "\n",
    "In 'local' mode, you can use your user credentials like AAD to access the [workspace](https://aka.ms/azureml/workspace) object. In 'remote' runs, it is recommended to use the Run Token for accessing Workspace resources. This can be abstracted through the `azureml.core.Run.get_context` method to obtain the Workspace object, which can then be used to access Datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabular Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'local'\n",
    "\n",
    "# get the workspace \n",
    "if run is 'local':\n",
    "    from azureml.core import Workspace\n",
    "    ws = Workspace.from_config()\n",
    "elif run is 'remote':\n",
    "    from azureml.core import Run\n",
    "    ws = Run.get_context().experiment.workspace\n",
    "\n",
    "# get the dataset \n",
    "dset = ws.datasets['my-data']\n",
    "\n",
    "# convert to PySpark dataframe\n",
    "df = dset.to_spark_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'remote'\n",
    "\n",
    "# get the workspace \n",
    "if run is 'local':\n",
    "    from azureml.core import Workspace\n",
    "    ws = Workspace.from_config()\n",
    "elif run is 'remote':\n",
    "    from azureml.core import Run\n",
    "    ws = Run.get_context().experiment.workspace\n",
    "\n",
    "# get the dataset \n",
    "dset = ws.datasets['my-data']\n",
    "\n",
    "# mount as hdfs (and get the path)\n",
    "path = dset.to_hdfs()\n",
    "\n",
    "# use normal Spark readers \n",
    "df = spark.sql.read_csv(f'{path}/*.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
